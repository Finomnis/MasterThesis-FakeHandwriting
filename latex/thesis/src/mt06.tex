\chapter{Writer Style Transfer}\label{chapter:writerStyleTransfer}

\section{Introduction}

\begin{wrapfigure}{r}{0.25\textwidth}
  \vspace{-15pt}
  \raggedleft
  %\fbox{%  
  \begin{minipage}{0.20\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../assets/showcase/style_transfer.pdf}
    \caption[Writer style transfer]{}
    \vspace{-45pt}
    \label{fig:writerStyleTransfer}
  \end{minipage}%}
\end{wrapfigure}

The purpose of this stage is to take handwriting in an online representation, analyze the handwriting style and then generate new handwriting with a given text content, as seen in \cref{fig:writerStyleTransfer}.

As this step mostly consists of existing work, we will not describe our methods at length.

\section{Methodology}

Producing realistic handwriting is a challenging task for neural networks. Online handwriting is a very good example of a problem that consists of both content and style, in a strongly interwoven way. The network therefore has to understand the difference between style and content while not overfitting on them. To produce convincing handwriting, it needs to reproduce the given content exactly, while keeping the style consistent, but not constant. Real human handwriting will repeat the same content almost identically, but still with some variance. This requires a solid long term memory, combined with some guidance from the required content.

While there are multiple ways to create neural networks that could deal with that kind of data, the most obvious choice is to utilize an \gls{rnn}. In particular, we looked into two \glspl{rnn}, one written by Alex Graves~\cite{graves}, which is the `original' network that demonstrated that generating sequences with complex long term dependencies is actually possible. The other network is \gls{deepwriting} by Aksan et al~\cite{deepwriting}.

\subsection{Graves' Handwriting Generation}
Graves' goal with his paper~\cite{graves} was not primarily handwriting generation, but instead to show that \gls{lstm} cells are capable of generating complex structures with long-term contextual dependencies. He used handwriting to demonstrate his technique, with remarkable success, as seen in \cref{fig:gravesOutputDemo}.

\begin{figure}
  \centering
  \includegraphics[width=0.65\textwidth]{../assets/style_transfer/graves_output_demo.pdf}
  \caption[Synthetic handwriting generated by Graves' network]{Synthetic handwriting generated by Graves' network. Source:~\cite{gravesImplementation}}
  \label{fig:gravesOutputDemo}
\end{figure}

He first started with pure pen position predictions, completely disconnected from the actual text content. He, therefore, defined a network that predicts the next pen position and pen-up events, based on all previous pen positions. The network itself is a fully connected network with multiple layers, where the input of every layer consists of the output of the parent layer (if a parent layer exists) and a skip connection from the raw input of the network. Further, every layer consists of \gls{lstm} cells and therefore also depends on its previous states, which enables the network to make predictions in context of the previous pen positions. The output of the network is fully connected to all intermediate layers. The skip connections from the input and to the output enable the network to decide by itself how shallow or deep the datapath should go, giving it full control over the functionality of the \gls{lstm} cells. A rough outline of the network can be seen in \cref{fig:gravesPreditionNetwork}.

\begin{figure}
  \centering
  \includegraphics[width=0.85\textwidth]{../assets/style_transfer/graves_prediction_network.pdf}
  \caption[Graves' prediction network architecture]{Graves' prediction network architecture. The circles represent network layers, the solid lines represent weighted connections and the dashed lines represent predictions. Source:~\cite{graves}}
  \label{fig:gravesPreditionNetwork}
\end{figure}

The output of the network consists of \emph{mixture densities}. This enables the network to express uncertainty, which is important in the case of handwriting, and enables variance, as the next sample can be generated by drawing from the mixtures densities.

This configuration already learned how to predict handwriting quite well. Nonetheless, its prediction was completely decoupled from the content as it had to predict both the content and the style simultaneously. Graves therefore refined his approach by adding the content of the text as a side input to one of the intermediate layers. He did not let the network see the entire content sequence at once, instead, he let the network itself decide what it wants to look at. To achieve this, he used another mixture density output from intermediate layers that would decide which part of the content gets delivered to the network. This would nowadays be called an \emph{attention} mechanism. The adjusted network can be seen in \cref{fig:gravesSynthesisNetwork}.


\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{../assets/style_transfer/graves_synthesis_network.pdf}
  \caption[Graves' synthesis network architecture]{Graves' synthesis network architecture. The difference to \cref{fig:gravesPreditionNetwork} is the extra input from the character sequence \textbf{c}, which gets fed to the network through a window layer. Source:~\cite{graves}}
  \label{fig:gravesSynthesisNetwork}
\end{figure}

Giving the network access to the content improved the predictions substantially, which was especially visible in the network output, as the variance of the mixture components reduced drastically. Especially between letters, the network became a lot more confident, as it now knows which letter follows instead of having to guess. Actually, the quality of the output got so much better that the network was now able to generate entire sequences on its own, instead of just predicting single steps. Generating sequences worked by feeding the prediction of the network back into it instead of using real pen positions.

This had the pleasant side effect that the network could now be \emph{primed} with a specific sequence, which is then continued by the network based on a given content. During the priming, the network implicitly stores the style of the priming sequence in its memory cells, and when asked to continue, it keeps writing in the same style, effectively doing a writer style transfer.

\subsection{DeepWriting}
\gls{deepwriting} built on Graves' idea of predicting single pen positions, but instead of relying on the internal memory of the network to store the style, their goal was to explicitly extract style and content from the input. They achieved that by utilizing a \gls{cvrnn} to split the input into two separate latent random variables, representing style and content.

Their approach is quite complex, and we will not detail it here. There are two big differences between Graves' network and \gls{deepwriting}: The \emph{input dataset annotations} and the \emph{handling of the text annotations}.

\textbf{Input dataset annotations.} Deepwriting needs the input strokes to be split into words and characters. Consequently, it requires the input data to contain \gls{bow} and \gls{eoc} labels. This is a big disadvantage of \gls{deepwriting}, as those labels have to be manually annotated and cannot be automatically determined from offline handwriting.

\textbf{Handling of text annotations.} While Graves' network implemented an attention mechanism to give it the freedom to figure out at what time which letter is important, \gls{deepwriting} uses the generated \gls{eoc} labels to switch letters. This removes some predictive capabilities of the network, as it can only see which letter follows as soon as the next letter is already about to be written. This is a clear disadvantage, as experiments in Graves' paper clearly show that the network decides to look at multiple letters at once and uses that information for smooth transitions between letters (\cref{fig:gravesTemporalMatching}).

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{../assets/style_transfer/graves_temporal_matching.pdf}
  \caption[The position of Graves' attention windows]{The position of Graves' attention windows. The horizontal axis represents the handwriting position, the vertical axis the accompanying text content position. The bright line is the position of the window function. The important detail to note is that the window doesn't only look at to the current letter, but also to the next and previous letters, which helps the network to make smoother transitions between letters. Source:~\cite{graves}}
  \label{fig:gravesTemporalMatching}
\end{figure}

\section{Evaluation}

\subsection{Implementation Details}
DeepWriting published its source code, so we used it without further modification.~\cite{deepwritingSource}

Graves' source, however, was quite outdated, so we took a modern, TensorFlow based implementation~\cite{gravesImplementation} and augmented it to our needs~\cite{gravesImplementationOurs}.

\subsection{Datasets}
For our initial testing, we used the handwriting dataset provided by the \gls{deepwriting} paper. This was necessary as it is the only available dataset with \gls{bow} and \gls{eoc} labels, as required by the \gls{deepwriting} network.

Later, we discarded the idea of utilizing the \gls{deepwriting} network and therefore also lost the necessity to use their dataset. We therefore switched to the IAM-Online dataset~\cite{iam-online} and, for full offline handwriting testing, the CVL dataset. ~\cite{cvl}

\subsection{Experiments}
As both networks already proved that they can handle synthesis and style transfer of online handwriting data, the real question is how the networks respond to replacing the real online input style with increasingly artificial variants. We conducted our testing in two steps. In both steps, we used synthetic online data as training samples, but first, we created the data from skeletonized real online date, and later, we created it using the entire previous pipeline on CVL-like data.

\subsubsection{Skeletonized online data}
The first step was to create artificial online data from skeletonized real online data. We used this step to compare the different resampling methods described in \cref{sec:resampling}.

For our first experiments, we utilized the \gls{deepwriting} network, until we encountered too many problems with it and decided to switch to Graves' network.

\textbf{No resampling.}
Using the generated online data without resampling didn't work for a multitude of reasons. For one, the data was too large, and the 16 GB RAM of the Tesla V100's we used were insufficient to actually train on the artificial online data without resampling it. But further, it is quite obvious why it would not have worked, even if we had the storage capacity: The network can only predict 8 possible outcomes: Horizontal, vertical and diagonal steps of one single pixel. As previously discussed, neural networks can easily deal with continuous, smooth data, but have big problems with sharp, high frequencies and their respective aliasing. This would have most likely prevented the network from converging.

\textbf{Constant velocity resampling.} 
With constant velocity resampling, we were now actually able to train the \gls{deepwriting} network. It converged, and learned to produce \gls{eoc} labels and strokes of roughly the correct length, but didn't actually grasp the concept of entire letters. This is supported by the fact that not all elements of the loss function converged.

In our search for the difference between the real online date (on which the network converged properly) and the generated online data, we found that the main problem was in fact the constant line segments, which removed an entire degree of freedom from the prediction capability of the network. It forced the output of the network to also consist of line segments of constant length, and the only thing that the network could still modify was the direction of those lines.
This led to the implementation of maximum acceleration resampling, as described previously.

\textbf{Maximum acceleration resampling.}
With maximum acceleration resampling, the network finally converged with all of its loss functions. While the output is still quite unreadable, first patterns of letters and even words showed up. While this is a big step in the right direction, it still is not good enough do be useful. Sadly, this is a strong indication that other problems with our generated online data still exist or that \gls{deepwriting} itself just isn't robust enough to deal with artificial data.

\textbf{Graves' network.}
As the difference between generated online data and real online data became increasingly difficult to tell, we decided to try the network of Graves as a reference, to see if the problem is actually our data or \gls{deepwriting} itself. To our surprise, Graves' network converged well and the output was readable and satisfactory. This made us discard the \gls{deepwriting} network along with its dataset.


\begin{figure}
  \centering
  \includegraphics[width=0.90\textwidth]{../assets/style_transfer/comparison.pdf}
  \caption[Comparison between handwriting style transfer methods]{Comparison between handwriting style transfer methods. \emph{Top left}: \gls{deepwriting} with constant velocity resampling. \emph{Top right}: \gls{deepwriting} with maximum acceleration resampling. \emph{Bottom left}: Graves' network with maximum acceleration resampling. \emph{Bottom right}: The real skeleton that was used for style input.}
  \label{fig:handwritingStyleTransferComparison}
\end{figure}

The results of the experiments can be seen in \cref{fig:handwritingStyleTransferComparison}.

\subsubsection{CVL data}
Sadly, the CVL dataset does not contain line-wise text annotations. Therefore we cannot train Graves' network with CVL directly. Luckily, we can use the IAM-Online~\cite{iam-online} dataset and the skeleton-to-cvl converter of \cref{chapter:skeletonization} to generate a CVL-like dataset with text annotations. We can then convert those images back to artificial online data to get samples that are very close to the ones we will get in the final pipeline.

An interesting question we explored was whether to train the handwriting generator with real or synthetic skeletons. The style input of the full pipeline will come from synthetic skeletons, therefore, it will be easier for the handwriting generator to extract the style if it was also trained on synthetic skeletons. On the other hand, synthetic skeletons always contain artifacts, which the generator will then also produce. Using real skeletons as training input for the generator could mitigate those artifacts, but during inference, it would increase the chance of failing to parse the style input entirely.

To answer this question, we trained the generator twice, once with real skeletons and once with the generated CVL-like skeletons. Then, we used the trained networks to synthesize new handwriting, once with real data as style input and once with synthetic data as style input, as seen in \cref{table:realSyntheticWriterStyleTransferComparison}.

\begin{table}
  \centering
  \begin{tabular}{lll}
  \toprule
  Training Data & Style Input & Result \\
  \midrule
  synthetic & synthetic & \raisebox{-0.4\height}{\includegraphics[scale=1.2]{../assets/style_transfer/real_synthetic_comparison/trained_reskeletonized_style_reskeletonized/synthetic_sample_lalign.pdf}} \\
  synthetic & real & \raisebox{-0.4\height}{\includegraphics[scale=1.2]{../assets/style_transfer/real_synthetic_comparison/trained_reskeletonized_style_online/synthetic_sample_lalign.pdf}} \\
  real & synthetic & \raisebox{-0.4\height}{\includegraphics[scale=1.2]{../assets/style_transfer/real_synthetic_comparison/trained_online_style_reskeletonized/synthetic_sample_lalign.pdf}} \\
  real & real & \raisebox{-0.4\height}{\includegraphics[scale=1.2]{../assets/style_transfer/real_synthetic_comparison/trained_online_style_online/synthetic_sample_lalign.pdf}} \\
  \midrule
  \multicolumn{2}{l}{Reference Style} & \raisebox{-0.4\height}{\includegraphics[scale=1.2]{../assets/style_transfer/real_synthetic_comparison/style.pdf}} \\
  \bottomrule
  \end{tabular}
  \caption[Performance evaluation of real vs synthesized training data]{Performance evaluation of real vs synthesized training data. We tested four times, with real and synthetic data both for the training dataset as also for the style input at evaluation time. Note that the network that was trained on real data produces a lot less artifacts, even if the style input is from synthetic data.}
  \label{table:realSyntheticWriterStyleTransferComparison}
\end{table}

The result were clear: The network trained on real data produced far fewer artifacts than the one trained with synthetic data. This was already expected, as the real training data also contains fewer artifacts than the synthetic training data, and after all, the network tries to recreate what it observes during training. Surprisingly, in this specific example, the network trained on real data seemed to perform better on synthetic style inputs than on real ones. This was consistent with other tests we ran, but we are not certain why this is the case.

Nonetheless, these results demonstrated that it is better to train the writer style transfer network on real skeleton data, even if it will receive synthetic skeletons as style inputs later.

\section{Results}

The results we achieved are quite satisfying. We successfully demonstrated that Graves' network is robust enough to adapt to our synthetic online data.

We demonstrate the ability of the trained network to extract and mimic unknown styles by generating texts from style samples of our test set, which the network has not seen during the training phase. The results can be seen in \cref{table:writerStyleTransferEvaluation}.


\begin{table}
  \centering
  \begin{tabular}{ll}
  \toprule
  Style Input & Output \\
  \midrule
  \raisebox{-0.4\height}{\includegraphics[scale=0.5]{../assets/style_transfer/style_transfer_in.pdf}} &
  \raisebox{-0.4\height}{\includegraphics[scale=0.5]{../assets/style_transfer/style_transfer_out.pdf}} \\
  \bottomrule
  \end{tabular}
  \caption[Qualitative evaluation of the writer style transfer]{Qualitative evaluation of the writer style transfer. The style inputs are from the test set, the network has not seen them in the training process.}
  \label{table:writerStyleTransferEvaluation}
\end{table}

\subsection{Typical Failure Modes}

However, the network still does make mistakes that clearly identify some of the results as artificially generated, as seen in \cref{fig:writerStyleTransferFailures}. 

The typical failure modes consist of:
\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item Addition of superfluous lines
\item Incorrect splitting of lines
\item Incorrect positioning of lines
\item Detail removal, like \emph{t}-, \emph{f}-lines or \emph{i}-dots 
\item Skipping letters at the start of the message, which is an artifact that happens when the network was unable to correctly parse the input style
\end{itemize}

Those failure cases clearly show that there is still room for improvement in future work. Nonetheless, the results are quite satisfactory for now.

\begin{figure}[H]
  \centering
  \vspace{0.05\textwidth}
  \includegraphics[width=0.65\textwidth]{../assets/style_transfer/failures.pdf}
  \caption[Typical failure modes of the writer style transfer]{Typical failure modes of the writer style transfer. (a) A redundant line was generated into the \emph{M}. (b) The vertical line of the \emph{I} is too short and placed incorrectly. (c) The \emph{j} is missing its dot. (d) The \emph{g} is broken into two parts that aren't properly connected.}
  \label{fig:writerStyleTransferFailures}
\end{figure}
